{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4\n",
    "\n",
    "\n",
    "## Viterbi Implementation\n",
    "\n",
    "This homework we'll be focusing on Viterbi Algorithm given an existing HMM. The goal of the homework is to find the likelihood of an utterance and the best   state sequence.\n",
    "\n",
    "* The observed features are 14-dimensional, real-valued speech features, from utterances of “yes”    and “no” sampled every 10 msec. In other words, each input line is a spe\n",
    "ech sample, 10 msec apart. (See the `input\\*.txt` files.)\n",
    "\n",
    "* Parameters (mean and variance vectors) of two “yes” and “no” GMMs are given . (See the `(mean|var)_\\*.txt` files.)\n",
    "\n",
    "* Transition probabilities of  “yes” and “no” HMMs are given. Each row is the transition probability for one state. For example, row 1 colum 3 is the transition probability from state 1 to state 3 (See the `trans_\\*.txt` files.)\n",
    "\n",
    "* Assume equal initial probabilities.\n",
    "\n",
    "* Assume that this is a whole-word recognizer, and that each word is recognized with a separate execution of the program. This will greatly simplify the implementation.\n",
    "\n",
    "* The transition probabilities <font color=red>are</font> in the log domain.\n",
    "\n",
    "* The mixture weights <font color=red>are NOT</font> in the log domain.\n",
    "\n",
    "* The covariance values <font color=red>are NOT </font>in the log domain. These covariance values are the diagonal of a 14 by 14 matrix.\n",
    "\n",
    "* <font color=red> The natural log (e) is used when computing log values. </font>\n",
    "\n",
    "* Your state sequence plot should look like to stateSeq.pdf.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What to turn in:\n",
    "\n",
    "1. Print out final likelihood scores and plot most likely state sequences for:\n",
    "\n",
    "        A) `input1.txt` given `hmm_yes`\n",
    "\n",
    "        B) `input1.txt` given `hmm_no`\n",
    "\n",
    "        C) `input2.txt` given `hmm_yes`\n",
    "\n",
    "        D) `input2.txt` given `hmm_no`\n",
    "\n",
    "        E) `input3.txt` given `hmm_yes`\n",
    "\n",
    "        F) `input3.txt` given `hmm_no`\n",
    "\n",
    "2. Use results to perform ASR…\n",
    "\n",
    "    A) is input1.txt more likely to be “yes” or “no”?\n",
    "\n",
    "    B) is input2.txt more likely to be “yes” or “no”?\n",
    "\n",
    "    C) is input3.txt more likely to be “yes” or “no”?\n",
    "\n",
    "\n",
    "Tip: Start small! This way you can always do some \"by hand\" calculations to verify what your code is doing.\n",
    "\n",
    "Tip: Don't forget about underflow! For this part of the assignment, it won't be a serious issue, but for the next part, it will be, so code accordingly. If you want, start with real-space probabilities, get things working that way, and then switch to log-probs.\n",
    "\n",
    "Tip: As you follow along with the algorithm from the book (or from the slides), do watch out for off-by-one errors. The book assumes 1-based indexing, whereas Python starts at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> self-assessment: </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
