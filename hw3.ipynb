{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you what use ngrams.py you should use python2\n",
    "# Or, otherwise, you need to modify ngrams.py by yourself in order to use it in python3.\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from ngrams import ngrams\n",
    "from collections import defaultdict\n",
    "from math import log, pow, inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_corpus = ['Why dont we start here',\n",
    "                  'Why dont we end there',\n",
    "                  'Let us start with a few other examples',\n",
    "                  'We never start with an example with so few tokens',\n",
    "                  'Tokens can be words that we start with in example docs'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Language Modeling\n",
    "For this part of the assignment, you will implement two simple count-based n-gram language models: one based on maximum-likelihood estimation, and another based on Witten-Bell smoothing. The data you will be using is a subset of the Penn Treebank's tagged Wall Street Journal articles on which we have done some initial processing. There are two versions of the data for this assignment:\n",
    "\n",
    "##### wsj.pos.gz\n",
    "##### wsj-normalized.pos.gz\n",
    "The difference is that, in the second (normalized) version of the data, we have collapsed some entries from certain tag categories (e.g. CDs, NNPs, etc.) into type-tokens to help reduce sparsity. Take a look at the data and see for yourself. Consider: what would be the benefits and drawbacks to this method of sparsity reduction? Note that, for this part of the assignment, the tags are un-necessary, so you'll want to work with the un-normalized version of the corpus.\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: produce a tag-free corpus\n",
    "\n",
    "For this task, you have two jobs. \n",
    "* First, you need to write a function to filter out all tags. \n",
    "* Second, Make sure your code works for both wsj.pos.gz and wsj-normalized.pos.gz\n",
    "\n",
    "####What to turn in\n",
    "* your code\n",
    "* some samples to show me that your code works as it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## WSJ.POS Examples########################\n",
      "b'Digital/NNP Equipment/NNP Corp./NNP reported/VBD a/DT 32/CD %/NN decline/NN in/IN net/JJ income/NN on/IN a/DT modest/JJ revenue/NN gain/NN in/IN its/PRP$ fiscal/JJ first/JJ quarter/NN ,/, causing/VBG some/DT analysts/NNS to/TO predict/VB weaker/JJR results/NNS ahead/RB than/IN they/PRP had/VBD expected/VBN ./.\\n'\n",
      "\n",
      "\n",
      "Digital Equipment Corp. reported a 32 % decline in net income on a modest revenue gain in its fiscal first quarter , causing some analysts to predict weaker results ahead than they had expected .\n",
      "\n",
      "b'Although/IN the/DT second-largest/JJ computer/NN maker/NN had/VBD prepared/VBN Wall/NNP Street/NNP for/IN a/DT poor/JJ quarter/NN ,/, analysts/NNS said/VBD they/PRP were/VBD troubled/VBN by/IN signs/NNS of/IN flat/JJ U.S./NNP orders/NNS and/CC a/DT slowdown/NN in/IN the/DT rate/NN of/IN gain/NN in/IN foreign/JJ orders/NNS ./. The/DT Maynard/NNP ,/, Mass./NNP ,/, company/NN is/VBZ in/IN a/DT transition/NN in/IN which/WDT it/PRP is/VBZ trying/VBG to/TO reduce/VB its/PRP$ reliance/NN on/IN mid-range/JJ machines/NNS and/CC establish/VB a/DT presence/NN in/IN workstations/NNS and/CC mainframes/NNS ./.\\n'\n",
      "\n",
      "\n",
      "Although the second-largest computer maker had prepared Wall Street for a poor quarter , analysts said they were troubled by signs of flat U.S. orders and a slowdown in the rate of gain in foreign orders . The Maynard , Mass. , company is in a transition in which it is trying to reduce its reliance on mid-range machines and establish a presence in workstations and mainframes .\n",
      "\n",
      "b'Net/NN for/IN the/DT quarter/NN ended/VBD Sept./NNP 30/CD fell/VBD to/TO $/$ 150.8/CD million/CD ,/, or/CC $/$ 1.20/CD a/DT share/NN ,/, from/IN $/$ 223/CD million/CD ,/, or/CC $/$ 1.71/CD a/DT share/NN ,/, a/DT year/NN ago/RB ./. Revenue/NN rose/VBD 6.4/CD %/NN to/TO $/$ 3.13/CD billion/CD from/IN $/$ 2.94/CD billion/CD ./.\\n'\n",
      "\n",
      "\n",
      "Net for the quarter ended Sept. 30 fell to $ 150.8 million , or $ 1.20 a share , from $ 223 million , or $ 1.71 a share , a year ago . Revenue rose 6.4 % to $ 3.13 billion from $ 2.94 billion .\n",
      "\n",
      "b\"Digital/NN said/VBD a/DT shift/NN in/IN its/PRP$ product/NN mix/NN toward/IN low-end/JJ products/NNS and/CC strong/JJ growth/NN in/IN workstation/NN sales/NNS yielded/VBD lower/JJR gross/JJ margins/NNS ./. A/DT spokesman/NN also/RB said/VBD margins/NNS for/IN the/DT company/NN 's/POS service/NN business/NN narrowed/VBD somewhat/RB because/IN of/IN heavy/JJ investments/NNS made/VBN in/IN that/DT sector/NN ./.\\n\"\n",
      "\n",
      "\n",
      "Digital said a shift in its product mix toward low-end products and strong growth in workstation sales yielded lower gross margins . A spokesman also said margins for the company 's service business narrowed somewhat because of heavy investments made in that sector .\n",
      "\n",
      "b\"The/DT lack/NN of/IN a/DT strong/JJ product/NN at/IN the/DT high/JJ end/NN of/IN Digital/NNP 's/POS line/NN was/VBD a/DT significant/JJ drag/NN on/IN sales/NNS ./. Digital/NNP hopes/VBZ to/TO address/VB that/DT with/IN the/DT debut/NN of/IN its/PRP$ first/JJ mainframe-class/JJ computers/NNS next/IN Tuesday/NNP ./. The/DT new/JJ line/NN is/VBZ aimed/VBN directly/RB at/IN International/NNP Business/NNP Machines/NNP Corp/NNP ./.\\n\"\n",
      "\n",
      "\n",
      "The lack of a strong product at the high end of Digital 's line was a significant drag on sales . Digital hopes to address that with the debut of its first mainframe-class computers next Tuesday . The new line is aimed directly at International Business Machines Corp .\n",
      "\n",
      "b\"``/`` Until/IN the/DT new/JJ mainframe/NN products/NNS kick/VBP in/RP ,/, there/EX wo/MD n't/RB be/VB a/DT lot/NN of/IN revenue/NN contribution/NN at/IN the/DT high/JJ end/NN ,/, and/CC that/DT 's/VBZ hurt/VB us/PRP ,/, ''/'' said/VBD Mark/NNP Steinkrauss/NNP ,/, Digital/NNP 's/POS director/NN of/IN investor/NN relations/NNS ./. He/PRP said/VBD unfavorable/JJ currency/NN translations/NNS were/VBD also/RB a/DT factor/NN in/IN the/DT quarter/NN ./.\\n\"\n",
      "\n",
      "\n",
      "`` Until the new mainframe products kick in , there wo n't be a lot of revenue contribution at the high end , and that 's hurt us , '' said Mark Steinkrauss , Digital 's director of investor relations . He said unfavorable currency translations were also a factor in the quarter .\n",
      "\n",
      "b\"DEC/NNP shares/NNS rose/VBD $/$ 1.375/CD to/TO $/$ 89.75/CD apiece/RB in/IN consolidated/JJ New/NNP York/NNP Stock/NNP Exchange/NNP trading/NN yesterday/NN ./. But/CC analysts/NNS said/VBD that/IN against/IN the/DT backdrop/NN of/IN a/DT nearly/RB 40-point/JJ rise/NN in/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/NNP ,/, that/WDT should/MD n't/RB necessarily/RB be/VB taken/VBN as/IN a/DT sign/NN of/IN great/JJ strength/NN ./. Some/DT cut/VBD their/PRP$ earnings/NNS estimates/NNS for/IN the/DT stock/NN this/DT year/NN and/CC predicted/VBD more/JJR efforts/NNS to/TO control/VB costs/NNS ahead/RB ./.\\n\"\n",
      "\n",
      "\n",
      "DEC shares rose $ 1.375 to $ 89.75 apiece in consolidated New York Stock Exchange trading yesterday . But analysts said that against the backdrop of a nearly 40-point rise in the Dow Jones Industrial Average , that should n't necessarily be taken as a sign of great strength . Some cut their earnings estimates for the stock this year and predicted more efforts to control costs ahead .\n",
      "\n",
      "b\"``/`` I/PRP think/VBP the/DT next/JJ few/JJ quarters/NNS will/MD be/VB difficult/JJ ,/, ''/'' said/VBD Steven/NNP Milunovich/NNP of/IN First/NNP Boston/NNP ./. ``/`` Margins/NNS will/MD remain/VB under/IN pressure/NN ,/, and/CC when/WRB the/DT new/JJ mainframe/NN does/VBZ ship/VB ,/, I/PRP 'm/VBP not/RB sure/JJ it/PRP will/MD be/VB a/DT big/JJ winner/NN ./. ''/'' Mr./NNP Milunovich/NNP said/VBD he/PRP was/VBD revising/VBG his/PRP$ estimate/NN for/IN DEC/NNP 's/POS current/JJ year/NN from/IN $/$ 8.20/CD a/DT share/NN to/TO ``/`` well/RB below/IN $/$ 8/CD ,/, ''/'' although/IN he/PRP has/VBZ n't/RB settled/VBN on/IN a/DT final/JJ number/NN ./.\\n\"\n",
      "\n",
      "\n",
      "`` I think the next few quarters will be difficult , '' said Steven Milunovich of First Boston . `` Margins will remain under pressure , and when the new mainframe does ship , I 'm not sure it will be a big winner . '' Mr. Milunovich said he was revising his estimate for DEC 's current year from $ 8.20 a share to `` well below $ 8 , '' although he has n't settled on a final number .\n",
      "\n",
      "######################## WSJ.NORMALIZED Examples:########################\n",
      "b'<NNP>/NNP <NNP>/NNP <NNP>/NNP reported/VBD a/DT <CD>/CD %/NN decline/NN in/IN net/JJ income/NN on/IN a/DT modest/JJ revenue/NN gain/NN in/IN its/PRP$ fiscal/JJ first/JJ quarter/NN ,/, causing/VBG some/DT analysts/NNS to/TO predict/VB weaker/JJR results/NNS ahead/RB than/IN they/PRP had/VBD expected/VBN ./.\\n'\n",
      "\n",
      "\n",
      "<NNP> <NNP> <NNP> reported a <CD> % decline in net income on a modest revenue gain in its fiscal first quarter , causing some analysts to predict weaker results ahead than they had expected .\n",
      "\n",
      "b'Although/IN the/DT second-largest/JJ computer/NN maker/NN had/VBD prepared/VBN <NNP>/NNP <NNP>/NNP for/IN a/DT poor/JJ quarter/NN ,/, analysts/NNS said/VBD they/PRP were/VBD troubled/VBN by/IN signs/NNS of/IN flat/JJ <NNP>/NNP orders/NNS and/CC a/DT slowdown/NN in/IN the/DT rate/NN of/IN gain/NN in/IN foreign/JJ orders/NNS ./. The/DT <NNP>/NNP ,/, <NNP>/NNP ,/, company/NN is/VBZ in/IN a/DT transition/NN in/IN which/WDT it/PRP is/VBZ trying/VBG to/TO reduce/VB its/PRP$ reliance/NN on/IN mid-range/JJ machines/NNS and/CC establish/VB a/DT presence/NN in/IN workstations/NNS and/CC mainframes/NNS ./.\\n'\n",
      "\n",
      "\n",
      "Although the second-largest computer maker had prepared <NNP> <NNP> for a poor quarter , analysts said they were troubled by signs of flat <NNP> orders and a slowdown in the rate of gain in foreign orders . The <NNP> , <NNP> , company is in a transition in which it is trying to reduce its reliance on mid-range machines and establish a presence in workstations and mainframes .\n",
      "\n",
      "b'Net/NN for/IN the/DT quarter/NN ended/VBD <NNP>/NNP <CD>/CD fell/VBD to/TO $/$ <CD>/CD <CD>/CD ,/, or/CC $/$ <CD>/CD a/DT share/NN ,/, from/IN $/$ <CD>/CD <CD>/CD ,/, or/CC $/$ <CD>/CD a/DT share/NN ,/, a/DT year/NN ago/RB ./. Revenue/NN rose/VBD <CD>/CD %/NN to/TO $/$ <CD>/CD <CD>/CD from/IN $/$ <CD>/CD <CD>/CD ./.\\n'\n",
      "\n",
      "\n",
      "Net for the quarter ended <NNP> <CD> fell to $ <CD> <CD> , or $ <CD> a share , from $ <CD> <CD> , or $ <CD> a share , a year ago . Revenue rose <CD> % to $ <CD> <CD> from $ <CD> <CD> .\n",
      "\n",
      "b\"Digital/NN said/VBD a/DT shift/NN in/IN its/PRP$ product/NN mix/NN toward/IN low-end/JJ products/NNS and/CC strong/JJ growth/NN in/IN workstation/NN sales/NNS yielded/VBD lower/JJR gross/JJ margins/NNS ./. A/DT spokesman/NN also/RB said/VBD margins/NNS for/IN the/DT company/NN 's/POS service/NN business/NN narrowed/VBD somewhat/RB because/IN of/IN heavy/JJ investments/NNS made/VBN in/IN that/DT sector/NN ./.\\n\"\n",
      "\n",
      "\n",
      "Digital said a shift in its product mix toward low-end products and strong growth in workstation sales yielded lower gross margins . A spokesman also said margins for the company 's service business narrowed somewhat because of heavy investments made in that sector .\n",
      "\n",
      "b\"The/DT lack/NN of/IN a/DT strong/JJ product/NN at/IN the/DT high/JJ end/NN of/IN <NNP>/NNP 's/POS line/NN was/VBD a/DT significant/JJ drag/NN on/IN sales/NNS ./. <NNP>/NNP hopes/VBZ to/TO address/VB that/DT with/IN the/DT debut/NN of/IN its/PRP$ first/JJ mainframe-class/JJ computers/NNS next/IN <NNP>/NNP ./. The/DT new/JJ line/NN is/VBZ aimed/VBN directly/RB at/IN <NNP>/NNP <NNP>/NNP <NNP>/NNP <NNP>/NNP ./.\\n\"\n",
      "\n",
      "\n",
      "The lack of a strong product at the high end of <NNP> 's line was a significant drag on sales . <NNP> hopes to address that with the debut of its first mainframe-class computers next <NNP> . The new line is aimed directly at <NNP> <NNP> <NNP> <NNP> .\n",
      "\n",
      "b\"``/`` Until/IN the/DT new/JJ mainframe/NN products/NNS kick/VBP in/RP ,/, there/EX wo/MD n't/RB be/VB a/DT lot/NN of/IN revenue/NN contribution/NN at/IN the/DT high/JJ end/NN ,/, and/CC that/DT 's/VBZ hurt/VB us/PRP ,/, ''/'' said/VBD <NNP>/NNP <NNP>/NNP ,/, <NNP>/NNP 's/POS director/NN of/IN investor/NN relations/NNS ./. He/PRP said/VBD unfavorable/JJ currency/NN translations/NNS were/VBD also/RB a/DT factor/NN in/IN the/DT quarter/NN ./.\\n\"\n",
      "\n",
      "\n",
      "`` Until the new mainframe products kick in , there wo n't be a lot of revenue contribution at the high end , and that 's hurt us , '' said <NNP> <NNP> , <NNP> 's director of investor relations . He said unfavorable currency translations were also a factor in the quarter .\n",
      "\n",
      "b\"<NNP>/NNP shares/NNS rose/VBD $/$ <CD>/CD to/TO $/$ <CD>/CD apiece/RB in/IN consolidated/JJ <NNP>/NNP <NNP>/NNP <NNP>/NNP <NNP>/NNP trading/NN yesterday/NN ./. But/CC analysts/NNS said/VBD that/IN against/IN the/DT backdrop/NN of/IN a/DT nearly/RB 40-point/JJ rise/NN in/IN the/DT <NNP>/NNP <NNP>/NNP <NNP>/NNP <NNP>/NNP ,/, that/WDT should/MD n't/RB necessarily/RB be/VB taken/VBN as/IN a/DT sign/NN of/IN great/JJ strength/NN ./. Some/DT cut/VBD their/PRP$ earnings/NNS estimates/NNS for/IN the/DT stock/NN this/DT year/NN and/CC predicted/VBD more/JJR efforts/NNS to/TO control/VB costs/NNS ahead/RB ./.\\n\"\n",
      "\n",
      "\n",
      "<NNP> shares rose $ <CD> to $ <CD> apiece in consolidated <NNP> <NNP> <NNP> <NNP> trading yesterday . But analysts said that against the backdrop of a nearly 40-point rise in the <NNP> <NNP> <NNP> <NNP> , that should n't necessarily be taken as a sign of great strength . Some cut their earnings estimates for the stock this year and predicted more efforts to control costs ahead .\n",
      "\n",
      "b\"``/`` I/PRP think/VBP the/DT next/JJ few/JJ quarters/NNS will/MD be/VB difficult/JJ ,/, ''/'' said/VBD <NNP>/NNP <NNP>/NNP of/IN <NNP>/NNP <NNP>/NNP ./. ``/`` Margins/NNS will/MD remain/VB under/IN pressure/NN ,/, and/CC when/WRB the/DT new/JJ mainframe/NN does/VBZ ship/VB ,/, I/PRP 'm/VBP not/RB sure/JJ it/PRP will/MD be/VB a/DT big/JJ winner/NN ./. ''/'' <NNP>/NNP <NNP>/NNP said/VBD he/PRP was/VBD revising/VBG his/PRP$ estimate/NN for/IN <NNP>/NNP 's/POS current/JJ year/NN from/IN $/$ <CD>/CD a/DT share/NN to/TO ``/`` well/RB below/IN $/$ <CD>/CD ,/, ''/'' although/IN he/PRP has/VBZ n't/RB settled/VBN on/IN a/DT final/JJ number/NN ./.\\n\"\n",
      "\n",
      "\n",
      "`` I think the next few quarters will be difficult , '' said <NNP> <NNP> of <NNP> <NNP> . `` Margins will remain under pressure , and when the new mainframe does ship , I 'm not sure it will be a big winner . '' <NNP> <NNP> said he was revising his estimate for <NNP> 's current year from $ <CD> a share to `` well below $ <CD> , '' although he has n't settled on a final number .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "#regex = re.compile(r\"/[A-Z$,.]{1,4}\") #stackoverflow says unnecessary, cached anyway\n",
    "maxln = 7\n",
    "\n",
    "def rem_tag(line):\n",
    "    line = line.decode('utf-8')\n",
    "    return re.sub(r\"/[A-Z$,.`']{1,4}\",'',line)\n",
    "\n",
    "print(\"######################## WSJ.POS Examples########################\")\n",
    "with gzip.open('wsj.pos.gz','r') as wsj:\n",
    "    count = 0\n",
    "    for line in wsj:\n",
    "        print(line)\n",
    "        print(\"\\n\")\n",
    "        line = rem_tag(line)\n",
    "        print(line)\n",
    "        count += 1\n",
    "        if count > maxln:\n",
    "            break\n",
    "\n",
    "print(\"######################## WSJ.NORMALIZED Examples:########################\")\n",
    "with gzip.open('wsj-normalized.pos.gz','r') as wsj:\n",
    "    count = 0\n",
    "    for line in wsj:\n",
    "        print(line)\n",
    "        print(\"\\n\")\n",
    "        line = rem_tag(line)\n",
    "        print(line)\n",
    "        count += 1\n",
    "        if count > maxln:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing Corpus\n",
    "wsj_corpus = []\n",
    "wsj_norm_corpus = []\n",
    "with gzip.open('wsj.pos.gz','r') as wsj:\n",
    "    for line in wsj:\n",
    "        line = rem_tag(line)\n",
    "        wsj_corpus += line.replace(',','').split(' .')\n",
    "\n",
    "with gzip.open('wsj-normalized.pos.gz','r') as wsj:\n",
    "    for line in wsj:\n",
    "        line = rem_tag(line)\n",
    "        wsj_norm_corpus += line.replace(',','').split(' .')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSJ: Net for the quarter ended Sept. 30 fell to $ 150.8 million  or $ 1.20 a share  from $ 223 million  or $ 1.71 a share  a year ago\n",
      "WSJ NORM:  Net for the quarter ended <NNP> <CD> fell to $ <CD> <CD>  or $ <CD> a share  from $ <CD> <CD>  or $ <CD> a share  a year ago\n"
     ]
    }
   ],
   "source": [
    "print(\"WSJ:\", wsj_corpus[5])\n",
    "print(\"WSJ NORM: \",wsj_norm_corpus[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "Now, start by producing code to compute maximum-likelihood estimate probabilities. Your code should be configurable with respect to the n-gram order- i.e., you should be able to set it to compute bigram, trigram, 4-gram, etc. probabilities. Refer to J&M and the lecture slides for definitions as needed. If you would like to write your own n-gram tokenization code, feel free to do so, but you may also use the ngrams.py utility class which contains a routine to take a list of tokens and produce a stream of n-grams with appropriate padding for the start and end of sentences.\n",
    "\n",
    "#### Tip: \n",
    "* Start with a very small \"toy\" corpus of just a couple of sentences for debugging. \n",
    "\n",
    "* As discussed in class, I strongly recommend using nested defaultdicts as the foundational data structure for your language model, where the \"outer\" key is the prefix, and the value retrieved by that prefix is a second defaultdict  containing possible suffices for that prefix, each of which is an \"inner\" key. E.g., p(\"TRUTHS\" | \"HOLD THESE\") would be retrieved by first looking up \"HOLD THESE\" and then from the resulting dictionary, looking up \"TRUTHS\": prob = trigrams[(\"HOLD\",\"THESE\")][\"TRUTHS\"] . Note that this arrangement makes it very easy to e.g. find out the number of times a given history occurs, the total probability mass assigned to all of a history's continuations, etc., all of which will be extremely helpful in the next part of the assignment.\n",
    "\n",
    "* Use tuples to represent prefixes. E.g., instead of the string \"HOLD THESE\", use the tuple (\"HOLD\", \"THESE\"). Note that, in Python, lists are mutable, and therefore may not be used as keys in dictionaries- but tuples are immutable, and so make excellent keys.\n",
    "\n",
    "* Don't forget about numerical underflow issues! You'll want to represent probabilities as negative base-2 log probabilities, and modify your arithmetic accordingly. I recommend experimenting with [the bitweight Python library](https://github.com/stevenbedrick/bitweight) (see its unit tests for example usage).\n",
    "* \n",
    "\n",
    "#### What to turn in:\n",
    "* your code \n",
    "* use your code to create a simple language model for small_corpus named as small_lm and show me that your output is correct(This is a small coupus so you could manully calculate the probalility).\n",
    "* use your code to create language model for wsj.pos.gz named as wsj_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=0 until smoothing: 0\n",
      "mle100 0.025\n",
      "=0 until smoothing: log:  inf decimal: 0.0\n",
      "mle log:  5.321928094887362 decimal prob: 0.025000000000000012\n",
      "'why dont we end there':mle log:  3.321928094887362 decimal prob: 0.1\n",
      "unigram:  us 5.285402218862249 decimal: 0.025641025641025633\n"
     ]
    }
   ],
   "source": [
    "def log2(x):\n",
    "    \"\"\"Return -log2(x)\"\"\"\n",
    "    if x == 0:\n",
    "        return inf\n",
    "    return -log(x, 2)\n",
    "\n",
    "def pow2(x):\n",
    "    \"\"\"Return 2^-x\"\"\"\n",
    "    return pow(2,-x)\n",
    "\n",
    "def make_dict(corpus, order):\n",
    "    \"\"\"Returns a nested defaultdict(defaultdict), formatted as follows:\n",
    "        [prefix]:[suffix]:prefix/suffix count\"\"\"\n",
    "    corpus_list = []\n",
    "    corpus_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    corpus_list += [corpus[i].lower().split() for i in range(len(corpus))] #corpus[i].lower().split() ?\n",
    "    chunks = ngrams(corpus_list, order)\n",
    "    lchunks = list(chunks)\n",
    "    \n",
    "    for i in range(len(lchunks)):\n",
    "        corpus_dict[lchunks[i][0] ][lchunks[i][1]] += 1\n",
    "\n",
    "    return corpus_dict\n",
    "\n",
    "def mle_solo(corp_dict, entry):\n",
    "    \"\"\"single n-gram probability function\n",
    "    Input: dictionary(dictionary()): corp_dict, String: sentence\n",
    "    Returns: ngrams as a list\n",
    "    Order identified from dictionary used\"\"\"\n",
    "\n",
    "    if entry[1] not in corp_dict[entry[0]]:\n",
    "        return 0\n",
    "    total = 0\n",
    "    for val in corp_dict[entry[0]]:\n",
    "        total += corp_dict[entry[0]][val]\n",
    "\n",
    "    #prob = corp_dict[entry[0]][entry[1]] / total\n",
    "    prob = log2(corp_dict[entry[0]][entry[1]]) - log2(total)\n",
    "    prob = pow2(prob)\n",
    "    return prob\n",
    "\n",
    "def mle100(corp_dict, sentence):\n",
    "    \"\"\"Multiply version of MLE, whole sentence (i.e. parsed through ngrams() for <S_0>, etc)\"\"\"\n",
    "    acc = 1\n",
    "    order = len(list(corp_dict.keys())[0]) + 1\n",
    "    n = list(ngrams([sentence.split()], order))\n",
    "    #print(n)\n",
    "    for entry in n:\n",
    "        if entry[1] not in corp_dict[entry[0]]:\n",
    "            return 0\n",
    "        total = 0\n",
    "        for i in corp_dict[entry[0]]:\n",
    "            total += corp_dict[entry[0]][i]\n",
    "        #print(total)\n",
    "        prob = corp_dict[entry[0]][entry[1]] / total\n",
    "        acc = acc * prob\n",
    "        #print(\"acc@\", entry[0], \":\", entry[1] ,acc)\n",
    "    return acc\n",
    "\n",
    "def mle_log(corp_dict, sentence):\n",
    "    \"\"\"Log version of MLE, returns in -log2\"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    acc = 0 #in log2\n",
    "    order = len(list(corp_dict.keys())[0]) + 1\n",
    "    n = list(ngrams([sentence.split()], order))\n",
    "    for entry in n:\n",
    "        total = 0\n",
    "        for i in corp_dict[entry[0]]:\n",
    "            total += corp_dict[entry[0]][i]\n",
    "        #print(\"total\",total)\n",
    "        if entry[1] not in corp_dict[entry[0]]:\n",
    "            return inf\n",
    "        suffix = corp_dict[entry[0]][entry[1]]\n",
    "        if (suffix == 0):\n",
    "            return inf\n",
    "        prob = log2(suffix) - log2(total)\n",
    "        #print(\"PROB\", prob)\n",
    "        assert prob >= 0, \"-log2(prob) not > 0\" #since negative log(fraction)\n",
    "        acc = acc + prob\n",
    "        #print(\"acc@\", entry[0], \":\", entry[1] ,acc)\n",
    "    return acc\n",
    "\n",
    "def print_dict(corpus_dict):\n",
    "    keys = corpus_dict.keys()\n",
    "    \n",
    "    for key in sorted(keys):\n",
    "        print(key)\n",
    "        for key2 in sorted(corpus_dict[key].keys()):\n",
    "            print(\"\\t\", key2, \":\", corpus_dict[key][key2])\n",
    "\n",
    "### Small Toy Test Case, BIGRAMS\n",
    "small_lm = make_dict(small_corpus, 2)\n",
    "\n",
    "### MLE with multiplication\n",
    "print(\"=0 until smoothing:\", mle100(small_lm, \"start\")) #0-probability case\n",
    "print(\"mle100\", mle100(small_lm, \"we start here\")) #non-zero probability case\n",
    "\n",
    "### MLE with log\n",
    "res1 = mle_log(small_lm, \"start\")\n",
    "print(\"=0 until smoothing: log: \", res1, \"decimal:\", pow2(res1)) #0-probability case\n",
    "res2 = mle_log(small_lm, \"we start here\") #non-zero probability case\n",
    "print(\"mle log: \", res2, \"decimal prob:\", pow2(res2))\n",
    "\n",
    "res3 = mle_log(small_lm, \"Why dont we end there\")\n",
    "print(\"'why dont we end there':mle log: \", res3, \"decimal prob:\", pow2(res3))\n",
    "\n",
    "small_lm1 = make_dict(small_corpus, 1)\n",
    "testword = \"us\"\n",
    "res = mle_log(small_lm1, testword)\n",
    "print(\"unigram: \",testword, res, \"decimal:\", pow2(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_lm = make_dict(wsj_corpus, 2)\n",
    "wsj_norm_lm = make_dict(wsj_norm_corpus, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle (negative) log:  16.0467925383373 decimal prob: 1.4771823564213143e-05\n",
      "mle (negative) log:  inf decimal prob: 0.0\n"
     ]
    }
   ],
   "source": [
    "res = mle_log(wsj_lm, \"In New York\") #non-zero probability case\n",
    "print(\"mle (negative) log: \", res, \"decimal prob:\", pow2(res))\n",
    "\n",
    "res = mle_log(wsj_lm, \"Pineapple\") #zero probability case\n",
    "print(\"mle (negative) log: \", res, \"decimal prob:\", pow2(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Once you’ve got an unsmoothed model working, move on to implementing Witten-Bell smoothing. Refer to the slides and J&M for details on how that ought to work.\n",
    "\n",
    "#### Tip: \n",
    "* You can modify an already-populated defaultdict to change its default value (for example, to store a default backoff value for a particular history) by changing the object’s default_factory attribute. Consult the documentation for examples of how this works.\n",
    "* As defined, W-B smoothing is highly recursive; you may find it more efficient to re-frame the algorithm in iterative terms.\n",
    "* As in the previous section, start small.\n",
    "* [This may offer you some help on how to implement Witten-Bell smoothing](http://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x207.html)\n",
    "\n",
    "\n",
    "#### What to turn in:\n",
    "* your code \n",
    "* use your code to create a simple smoothed language model based on small_lm  and show me that your output is correct(This is a small coupus so you could manully calculate the probalility).\n",
    "* use your code to create a smoothed language model based on wsj_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6 6 0 5\n",
      "15 0.5 0\n",
      "6 6 0 5\n",
      "15 0.5 0\n"
     ]
    }
   ],
   "source": [
    "#Default value usage test>Pass\n",
    "d1 = defaultdict(int)\n",
    "d1[\"apple\"] = 5\n",
    "print(d1[5])\n",
    "x = 6\n",
    "d1.default_factory = lambda x=x: x #6\n",
    "print(d1.default_factory(), d1[100], d1[5], d1[\"apple\"]) #Result 6 6 0 5\n",
    "#Note: d1[5] set to initial value 0 after first call\n",
    "#Note: Use d1.default_factory() to call lambda function directly without creating new entry\n",
    "\n",
    "d2 = defaultdict(lambda: defaultdict(int))\n",
    "d2[1][2] = 15\n",
    "x = 0.5\n",
    "d2[1].default_factory = lambda x=x: x #0.5\n",
    "print(d2[1][2], d2[1][111], d2[2][2]) #Expected 15, 0.5, 0\n",
    "\n",
    "#Note: Changing lambda: 6 to lambda: x causes an issue.\n",
    "#https://stackoverflow.com/questions/10452770/python-lambdas-binding-to-local-values\n",
    "#https://stackoverflow.com/questions/21053988/lambda-function-acessing-outside-variable\n",
    "print(d1.default_factory(), d1[100], d1[5], d1[\"apple\"])\n",
    "print(d2[1][2], d2[1][111], d2[2][2])\n",
    "\n",
    "#Note: Use lambda x=x: x to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 2\n",
    "# models = [] #list of dict(dict(ints)); each entry is a 1-gram, 2-gram, ..., n-gram\n",
    "\n",
    "############## Small Toy Test Case\n",
    "\n",
    "#Initialize n-gram dictionaries from unigram to desired n-gram\n",
    "# for i in range(n):\n",
    "#     small_lm = make_dict(small_corpus, i+1)\n",
    "#     models.append(small_lm.copy())\n",
    "# #Verify\n",
    "# #print_dict(models[n-1])\n",
    "\n",
    "# #Build up lambda terms for unigram to (n)-gram, storing in models[]\n",
    "# for i in range(0,n):\n",
    "#     keys = models[i].keys()\n",
    "#     print(\"KEYS: \", keys)\n",
    "#     for key in keys:\n",
    "#         inner_keys = models[i][key].keys()\n",
    "#         print(\"INNER KEYS: \", inner_keys)\n",
    "        \n",
    "#         tokens = 0\n",
    "#         for ikey in inner_keys:\n",
    "#             tokens += models[i][key][ikey]\n",
    "\n",
    "#         types = 0\n",
    "#         types = len(inner_keys)\n",
    "        \n",
    "#         print(\"(tokens types)\", tokens, types)\n",
    "#         lamb = tokens / (tokens+types)\n",
    "#         models[i][key].default_factory = lambda lamb=lamb: lamb\n",
    "\n",
    "        #print(\"lambda for n =\",(i+1),\"key=\",key,\":\", models[i][key].default_factory())\n",
    "\n",
    "def make_lamb_dicts(models, n, corpus, debug = 0):\n",
    "    #Initialize n-gram dictionaries from unigram to desired n-gram\n",
    "    for i in range(n):\n",
    "        lm = make_dict(corpus, i+1)\n",
    "        models.append(lm.copy())\n",
    "    #Verify\n",
    "    #print_dict(models[n-1])\n",
    "\n",
    "    #Build up lambda terms for unigram to (n)-gram, storing in models[]\n",
    "    for i in range(0,n):\n",
    "        keys = models[i].keys()\n",
    "        if debug:\n",
    "            print(\"KEYS: \", keys)\n",
    "        for key in keys:\n",
    "            inner_keys = models[i][key].keys()\n",
    "            if debug:\n",
    "                print(\"INNER KEYS: \", inner_keys)\n",
    "\n",
    "            tokens = 0\n",
    "            for ikey in inner_keys:\n",
    "                tokens += models[i][key][ikey]\n",
    "\n",
    "            types = 0\n",
    "            types = len(inner_keys)\n",
    "            \n",
    "            if debug:\n",
    "                print(\"(tokens types)\", tokens, types)\n",
    "            lamb = tokens / (tokens+types)\n",
    "            models[i][key].default_factory = lambda lamb=lamb: lamb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 3 #up to trigrams created\n",
    "testlambs = []\n",
    "make_lamb_dicts(testlambs, n, small_corpus, debug = 0)\n",
    "#print_dict(testlambs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -grams: [((), 'let')]\n",
      "unigram smoothed:  let 0.03125\n",
      "uni_tokens, uni_types: 39 25\n",
      "lambda.dot =  0.609375\n",
      "unseen 0.015625\n",
      "2 -grams: [(('<S_0>',), 'let'), (('let',), 'us'), (('us',), '</S_0>')]\n",
      "current gram:  (('<S_0>',), 'let')\n",
      "s:  [('<S_0>',), 'let'] ptemp:  0.12500000000000003\n",
      "mle [('<S_0>',), 'let'] 0.2\n",
      "current gram:  (('let',), 'us')\n",
      "s:  [('let',), 'us'] ptemp:  0.515625\n",
      "mle [('let',), 'us'] 1.0\n",
      "current gram:  (('us',), '</S_0>')\n",
      "s:  [('us',), '</S_0>'] ptemp:  0.0078125\n",
      "mle [('us',), '</S_0>'] 0\n",
      "unigram smoothed:  let us 0.0005035400390625003\n",
      "2 -grams: [(('<S_0>',), 'why'), (('why',), 'why'), (('why',), '</S_0>')]\n",
      "bigram unseen pair smoothed:  why why 1.9779911747685202e-05\n",
      "2 -grams: [(('<S_0>',), 'us'), (('us',), 'with'), (('with',), '</S_0>')]\n",
      "bigram unseen pair smoothed:  us with 4.23855251736111e-06\n",
      "2 -grams: [(('<S_0>',), 'chicken'), (('chicken',), 'chicken'), (('chicken',), '</S_0>')]\n",
      "bigram unseen tokens smoothed:  chicken chicken 1.6954210069444448e-06\n",
      "unigram unsmoothed:  let 5.285402218862249 decimal: 0.025641025641025633\n",
      "2 -grams: [(('<S_0>',), 'why'), (('why',), 'dont'), (('dont',), 'we'), (('we',), 'end'), (('end',), 'there'), (('there',), '</S_0>')]\n",
      "bigram completely seen sentence:  why dont we end there 0.004699843648941842\n",
      "3 -grams: [(('<S_0>', '<S_1>'), 'why'), (('<S_1>', 'why'), 'dont'), (('why', 'dont'), 'we'), (('dont', 'we'), 'end'), (('we', 'end'), 'there'), (('end', 'there'), '</S_1>'), (('there', '</S_1>'), '</S_0>')]\n",
      "trigram completely seen sentence:  why dont we end there 0.01241116572933721\n",
      "3 -grams: [(('<S_0>', '<S_1>'), 'let'), (('<S_1>', 'let'), 'us'), (('let', 'us'), 'start'), (('us', 'start'), 'here'), (('start', 'here'), '</S_1>'), (('here', '</S_1>'), '</S_0>')]\n",
      "trigram sentence:  let us start here 0.0016515771403646793\n"
     ]
    }
   ],
   "source": [
    "#Calculate Smoothed Probability, starting with unigram level (iterative)\n",
    "#list_grams = list(ngrams([test_case.split()], 2))\n",
    "#print(list_grams)\n",
    "\n",
    "def wbSmooth(list_dicts, sentence, order, debug = 0):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        list_dicts: List of nested defaultdict(defaultdict(int)) structure containing n-gram counts\n",
    "            from 1-gram to (order)-gram.\n",
    "        sentence: Input sentence to calculate probability on\n",
    "        order: degree of n-gram. Must be within provided (list_dict) range\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    sentence=sentence.lower()\n",
    "\n",
    "    uni_tokens = sum(list_dicts[0][()].values())\n",
    "    uni_types = len(list_dicts[0][()].keys())\n",
    "    lamb_dot = uni_tokens / (uni_tokens+uni_types)\n",
    "    unseen = 1/(uni_tokens+uni_types)\n",
    "    if debug:\n",
    "        print(\"uni_tokens, uni_types:\", uni_tokens, uni_types)\n",
    "        print(\"lambda.dot = \", lamb_dot)\n",
    "        print(\"unseen\", unseen)\n",
    "    \n",
    "    grams = list(ngrams([sentence.split()], order))\n",
    "    print(order,\"-grams:\", grams)\n",
    "    probs = []\n",
    "\n",
    "#Calculate smoothed prob for every n-gram in (sentence) and add to accumulator\n",
    "    for gram in grams:\n",
    "        prefix = gram[0]\n",
    "        suffix = gram[1]\n",
    "        if debug:\n",
    "            print(\"current gram: \", gram)\n",
    "\n",
    "        ptemp =0.5\n",
    "\n",
    "        for i in range(0,order):\n",
    "            if i == 0: #base case, unigrams\n",
    "                #p = lamb_dot * pow2(mle_log(list_dicts[i], suffix)) + unseen\n",
    "                p = lamb_dot * (mle100(list_dicts[i], suffix)) + unseen\n",
    "                #p = pow2( log2(lamb_dot) + log2((mle100(list_dicts[i], suffix))) ) + unseen #no effect\n",
    "                ptemp = p\n",
    "            else:\n",
    "                gram_dict = list_dicts[i][prefix[order-i-1:]] #slice of prefix for current level of n-gram\n",
    "                lamb = gram_dict.default_factory()\n",
    "                #s = \" \".join(prefix[order-i-1:])+\" \"+suffix #mle function takes [prefix, suffix]\n",
    "                s = [prefix[order-i-1:], suffix]\n",
    "                mle = mle_solo(list_dicts[i], s)\n",
    "                #ptemp = lamb*mle + (1-lamb)*ptemp\n",
    "                ptemp = pow2(log2(lamb)+log2(mle)) + pow2(log2((1-lamb))+log2(ptemp))\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"s: \",s, \"ptemp: \", ptemp)\n",
    "                    print(\"mle\", s, mle_solo(list_dicts[i], s))\n",
    "            \n",
    "            #print(\"prefix\", (gram[order-2-i],) ) #Unigram prefix\n",
    "        probs.append( ptemp)\n",
    "#Multiply (log form) individual gram probabilities together\n",
    "    ret = 0\n",
    "    for prob in probs:\n",
    "        ret += log2(prob)\n",
    "    return pow2(ret)\n",
    "\n",
    "test_case = \"let\"\n",
    "print(\"unigram smoothed: \",test_case, wbSmooth(testlambs ,test_case, 1))\n",
    "test_case = \"let us\"\n",
    "print(\"unigram smoothed: \",test_case, wbSmooth(testlambs ,test_case, 2, debug=1))\n",
    "test_case = \"why why\"\n",
    "print(\"bigram unseen pair smoothed: \",test_case, wbSmooth(testlambs ,test_case, 2, debug=0))\n",
    "test_case = \"us with\"\n",
    "print(\"bigram unseen pair smoothed: \",test_case, wbSmooth(testlambs ,test_case, 2, debug=0))\n",
    "test_case = \"chicken chicken\"\n",
    "print(\"bigram unseen tokens smoothed: \",test_case, wbSmooth(testlambs ,test_case, 2, debug=0))\n",
    "\n",
    "testword = \"let\"\n",
    "res = mle_log(small_lm1, testword)\n",
    "print(\"unigram unsmoothed: \",testword, res, \"decimal:\", pow2(res))\n",
    "\n",
    "#Bigger test cases\n",
    "test_case = \"why dont we end there\"\n",
    "print(\"bigram completely seen sentence: \",test_case, wbSmooth(testlambs ,test_case, 2, debug=0))\n",
    "#Trigrams test for fun\n",
    "test_case = \"why dont we end there\"\n",
    "print(\"trigram completely seen sentence: \",test_case, wbSmooth(testlambs ,test_case, 3, debug=0))\n",
    "test_case = \"let us start here\"\n",
    "print(\"trigram sentence: \",test_case, wbSmooth(testlambs ,test_case, 3, debug=0))\n",
    "\n",
    "#print_dict(testlambs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying small corpus by hand:\n",
    "lambda.dot = tokens/(tokens+types) = 39/(39+25) = 39/64\n",
    "\n",
    "Unigram \"let\" occurs once in the corpus of 39 tokens\n",
    "\n",
    "therefore, P.wb(let) = (39/64) * (1/39) + (1/64) = **2/64**, \n",
    "\n",
    "\n",
    "consistent with the result above `unigram smoothed:  let 0.03125`\n",
    "\n",
    "The Bigram of `<S_0>` let, is then equivalent to (5/9) * (1/5) + (4/9) * (2/64) = **0.125**\n",
    "with lambda of `<S_0>` = 5/9\n",
    "MLE(let|`<S_0>` ) = 1/5\n",
    "\n",
    "consistent with the result above `s:  [('<S_0>',), 'let'] ptemp:  0.125`\n",
    "\n",
    "### Analysis\n",
    "The following test cases and outputs taken from above:\n",
    "\n",
    "The results are as expected. As we move from a sentence with more bigrams in the corpus (e.g. for \"let us\", both 'S_0 let' AND 'let us' occur in the corpus) to one with 0-bigrams, but still with unigrams in the corpus (\"us with\"), to 0 sightings in the corpus at all (\"chicken chicken\"), the probability goes down.\n",
    "\n",
    "    Input            Probability         Links\n",
    "    let us           5.035 e-04          2/3 bigrams exist, 2/2 unigrams exist\n",
    "    why why          1.977 e-05          1/3 bigrams exist, 2/2 unigrams exist\n",
    "    us with          4.23 e-06           0/3 bigrams exist, 2/2 unigrams exist\n",
    "    chicken chicken  1.69 e-06           0/3 bigrams exist, 0/2 unigrams exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation via Perplexity\n",
    "Explore the effects of n-gram order using perplexity. Perform ten-fold cross-validation on the WSJ corpus. On each iteration, this will give you a different 90/10 training/test split; train a smoothed language model on the 9 training sections, and compute the average per-token perplexity of the tenth section. The slides from the language modeling lecture give the equation for perplexity computation (as does J&M chapter 4); you'll need to modify the equation a bit, since we're using log-probabilities. \n",
    "\n",
    "Now, try this for unigram, bigram, trigram, and 4-gram models. \n",
    "\n",
    "#### What to turn in\n",
    "* your cross-validation function. You are not suppose to use any cross-validation function from any module. You should implement it by yourself.\n",
    "* your perplexity function\n",
    "* cross-validation result for unigram, bigram, trigram, and 4-gram models on wsj.pos.gz\n",
    "* cross-validation result for unigram, bigram, trigram, and 4-gram models on wsj-normalized.pos.gz.\n",
    "* Answer following 2 questions: \n",
    "    * How does perplexity change as the model order size increases?\n",
    "    * How does perplexity change as the data changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
